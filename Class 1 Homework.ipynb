{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MCP Homework and Local LLM Implementation with Ollama & LangChain**\n",
    "*In this thrust we have four tasks, which will be enhancing MCP using and learning how to use two brilliant tools for LLM application.*\n",
    "\n",
    "Prework:  follow the instruction in slides.Implement the MCP in Local Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Using MCP to built Agent-like work-flow.\n",
    "\n",
    "**Task 1.1 MCP + Claude = Browser Automation**\\\n",
    "*use MCP + Claude to do browser automation tasks*\n",
    "step 1: Learn all kinds of cool MCP server\\\n",
    "[Introducing to more MCP server](https://github.com/punkpeye/awesome-mcp-servers)\n",
    "\n",
    "step 2: Install cline\\\n",
    "[cline](https://cline.bot/)\n",
    "\n",
    "step 3: Open Claude, follow the instruction in the MCP Configuration file(mcp_example.json). Use rave Search„ÄÅGitHub„ÄÅPuppeteer„ÄÅFilesystem„ÄÅSequential Thinking and Notion.\n",
    "\n",
    "Now, follow the instructions below to complete a small task for each plugin.\n",
    "\n",
    "1. üîç Use Brave Search to:\n",
    "Task: Search for ‚Äúlatest AI paper publication platforms‚Äù and list the top 3 search results with titles and URLs.\n",
    "Prompt in Claude:\n",
    "\"Use Brave Search to look up the latest AI paper publication platforms and return the top 3 results with title and link.\"\n",
    "\n",
    "2. üíº Use GitHub to:\n",
    "Task: Access one of your public repositories (e.g., my-cool-project) and list the 5 most recent commits.\n",
    "Prompt in Claude:\n",
    "\"Connect to my GitHub account using the MCP plugin and list the 5 latest commits from the repository my-cool-project.\"\n",
    "\n",
    "3. ü§ñ Use Puppeteer to:\n",
    "Task: Visit https://www.inference.ai/, take a full-page screenshot, and save it as example.png.\n",
    "Prompt in Claude:\n",
    "\"Use Puppeteer to go to https://www.inference.ai/ and capture a full-page screenshot saved as example.png.\"\n",
    "\n",
    "4. üíæ Use Filesystem to:\n",
    "Task: Create a new folder on your Desktop named mcp_test, and inside it, create a text file hello.txt containing ‚ÄúHello MCP!‚Äù.\n",
    "Prompt in Claude:\n",
    "\"Use Filesystem to create a folder named mcp_test on my Desktop and add a file hello.txt inside with the text 'Hello MCP!'.\"\n",
    "\n",
    "5. üß† Use Sequential Thinking to:\n",
    "Task: Think step-by-step about how to prepare for a technical interview and generate a preparation plan.\n",
    "Prompt in Claude:\n",
    "\"Use Sequential Thinking to create a step-by-step plan for preparing for a technical interview.\"\n",
    "\n",
    "6. üìù Use Notion to:\n",
    "Task: Create a new Notion page titled ‚ÄúMCP Automation Test‚Äù and log the results of all the tasks above.\n",
    "Prompt in Claude:\n",
    "\"Use the Notion plugin to create a new page titled 'MCP Automation Test' and write a summary of the tasks I just completed using each plugin.\"\n",
    "\n",
    "\n",
    "Advanced Task: Use Claude + Puppeteer to automatically visit a webpage, scrape table content, and save it locally (with the help of the Filesystem plugin).\n",
    "Project Management Workflow: Record the scraped and analyzed data into a Notion database, automatically generating documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Play with Ollama\n",
    "\n",
    "<img src=\"https://ollama.com/public/blog/meta-ollama-llama3.png\" alt=\"jupyter\" width=\"500\"/>\n",
    "\n",
    "***Ollama** is a **convenient** and **free** frameworkÔºådesigned for easy deployment and running of large language models (LLMs) locally.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 2.1: Install Ollama and run LLMs locally**  \n",
    "- refer to [Ollama](https://ollama.ai/) to complete installation.  \n",
    "- Run `ollama run llama2` from the command line to download and launch the `llama2` model.\n",
    "\n",
    "\n",
    "**Task 2.2: Using Ollama to call OpenAI API**\\\n",
    "*Ollama now has built-in compatibility with the OpenAI Chat Completions API, making it possible to use more tooling and applications with Ollama locally.*\n",
    "\n",
    "See official instruction belowÔºö\\\n",
    "[Ollama OpenAI Compatibility](https://ollama.com/blog/openai-compatibility)\\\n",
    "[Ollama OpenAI](https://github.com/ollama/ollama/blob/main/docs/openai.md)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UsageÔºö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To invoke Ollama‚Äôs OpenAI compatible API endpoint, use the same OpenAI format and change the hostname to http://localhost:11434:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curl Method:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curl http://localhost:11434/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"llama2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello!\"\n",
    "            }\n",
    "        ]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In above examples, ‚ÄúOllama‚Äù is essentially acting as a local server that is compatible with the OpenAI API. In other words, the endpoint you‚Äôre calling‚Äîwhether via code or a cURL command‚Äîis not the official OpenAI endpoint at https://api.openai.com/v1/ but rather http://localhost:11434/v1/. The local process running on this port is Ollama.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Combining LangChain with Ollama‚Äôs local LLMs\n",
    "\n",
    "*LangChain simplifies every stage of the LLM application lifecycle.\n",
    "It unifies functional modules such as \"Prompt design\", \"Multi-round dialogue memory (Memory)\", \"External data retrieval (Retrieval)\" and \"Tools/Agents\" into a unified package. In this way, you do not need to manually manage each step of the language model call and data flow, and only need to focus on business logic.*\n",
    "\n",
    "LangChain‚ÄìOllama Documentation: [https://python.langchain.com/docs/integrations/llms/ollama](https://python.langchain.com/docs/integrations/llms/ollama)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*-PlFCd_VBcALKReO3ZaOEg.png\" alt=\"jupyter\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1 Reproduce practice in lecture using LCEL**\n",
    "\n",
    "*A chain is a sequence of steps or model calls connected together to achieve a larger task. Each step can involve retrieving information, transforming text, or invoking a language model in some way, and then passing its output on to the next step in the chain. This structure helps you build more complex workflows or pipelines using multiple actions in a simple, organized manner.*\n",
    "\n",
    "- what is LCEL? LCEL is a much simpler way to construct \"Chain\"\\\n",
    "[LCEL](https://python.langchain.com/docs/concepts/lcel/)\\\n",
    "why use it?\\\n",
    "[Why LCEL](https://python.langchain.com/v0.1/docs/expression_language/why/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code with a Ollama local modelÔºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using LCEL to reproduce a \"Basic Prompting\" scenario\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama \n",
    "\n",
    "# 2. Define the prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the capital of {topic}?\"\n",
    ")\n",
    "\n",
    "# 3. Define the model\n",
    "model = ChatOllama(model = [\"llama2\"])  # Using Ollama \n",
    "\n",
    "# 4. Chain the components together using LCEL\n",
    "chain = (\n",
    "    # LCEL syntax: use the pipe operator | to connect each step\n",
    "    {\"topic\": RunnablePassthrough()}  # Accept user input\n",
    "    | prompt                          # Transform it into a prompt message\n",
    "    | model                           # Call the model\n",
    "    | StrOutputParser()               # Parse the output as a string\n",
    ")\n",
    "\n",
    "# 5. Execute\n",
    "result = chain.invoke(\"Germany\")\n",
    "print(\"User prompt: 'What is the capital of Germany?'\")\n",
    "print(\"Model answer:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Submission Requirements**  \n",
    "1. Complete all tasks.  \n",
    "2. Include screenshots of key outputs (e.g., model responses, agent computation results).\n",
    "\n",
    "- Advance WorkÔºöIntegrate the Ollama and Langchain tasks into **Gradio Web UI**, which will be useful for building Proxy AI-Agent interface translation with front-end, and demonstrate your work.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
